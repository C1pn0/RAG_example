{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyMaysksr+6DxddWI+qz3grU"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": "!pip install langchain biopython pubmed_parser pinecone-client openai tiktoken langchain_pinecone gradio",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nf1nYgC4C4e5",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1714464938542,
     "user_tz": -180,
     "elapsed": 34487,
     "user": {
      "displayName": "Никита Бобылев",
      "userId": "16665361595543873478"
     }
    },
    "outputId": "d7a24c92-63c3-4162-df37-44c7ebc38b66"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Import necessary libraries for the notebook\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "import os, time\n",
    "from Bio import Entrez\n",
    "import pubmed_parser as pp\n",
    "from pinecone import Pinecone, ServerlessSpec"
   ],
   "metadata": {
    "id": "gdMyIUOC2Z8d",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1714468218561,
     "user_tz": -180,
     "elapsed": 2,
     "user": {
      "displayName": "Никита Бобылев",
      "userId": "16665361595543873478"
     }
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Initiate the Pinecone client instance\n",
    "clientPine = Pinecone(api_key=os.environ['PINECONE_API_KEY'])"
   ],
   "metadata": {
    "id": "R-7Mku4b--PE",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1714466311171,
     "user_tz": -180,
     "elapsed": 1909,
     "user": {
      "displayName": "Никита Бобылев",
      "userId": "16665361595543873478"
     }
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "###### Data Fetching and Processing\n",
    "* Fetch and process PubMed data based on a provided query. This code extracts publication data using the PubMed ID (PMID), then processes multiple PMIDs to gather specified fields.\n",
    "*  Utilizes the Biopython Entrez API to search PubMed and fetch details for each PMID. Results are processed to extract relevant keys (pmid, title, abstract) and saved locally as data.json.\n",
    "*  This function sets the groundwork for analyzing or utilizing PubMed data, critical for subsequent data manipulation and embedding."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def fetch_pubmed_data(pmid, keys):\n",
    "    print(pmid)\n",
    "    data = pp.parse_xml_web(pmid=pmid)  # Ensure this function can be awaited or adapt as necessary\n",
    "    return {key: data.get(key.lower(), None) for key in keys}\n",
    "\n",
    "def process_pmids(pmids, keys):\n",
    "    results = [fetch_pubmed_data(pmid, keys) for pmid in pmids]\n",
    "    return results\n",
    "\n",
    "def parse_data(query):\n",
    "    Entrez.email = os.environ['DEFAULT_EMAIL']\n",
    "    with Entrez.esearch(db='pubmed', term=query, retmax=50) as handle:\n",
    "        pmid_list = Entrez.read(handle).get('IdList')\n",
    "        handle.close()\n",
    "\n",
    "    keys = ['pmid', 'title', 'abstract']\n",
    "    results = process_pmids(pmid_list, keys)\n",
    "    for result in results:\n",
    "        print(result, type(results))\n",
    "    with open('data.json', 'w') as f:\n",
    "        f.writelines([str(result) + '\\n' for result in results])\n",
    "    return results"
   ],
   "metadata": {
    "id": "xU4ip3114sN0",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1714466487039,
     "user_tz": -180,
     "elapsed": 290,
     "user": {
      "displayName": "Никита Бобылев",
      "userId": "16665361595543873478"
     }
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Data Parsing and Extraction\n",
    "Executes the data fetching function with a specific query and extracts titles, abstracts and PMIDs from the results. This data is then used to generate embeddings for each abstract, which are stored in a Pinecone index."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "data = parse_data(query='\"meta analysis\"[Publication Type]')\n",
    "\n",
    "texts = [item['abstract'] for item in data]\n",
    "pmids = [item['pmid'] for item in data]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XHT_lSuJ3Fa1",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1714466515312,
     "user_tz": -180,
     "elapsed": 26688,
     "user": {
      "displayName": "Никита Бобылев",
      "userId": "16665361595543873478"
     }
    },
    "outputId": "fed5bcc6-1b5f-4b76-d4b5-a6a454dcc465"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Embedding Text Data\n",
    "\n",
    "Embeds the extracted text data (abstracts) using an OpenAI model. The resulting embeddings are then stored in a Pinecone index for efficient retrieval and similarity search."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "response = OpenAIEmbeddings(\n",
    "  model='text-embedding-ada-002'\n",
    ")\n",
    "embedded = response.embed_documents(texts)"
   ],
   "metadata": {
    "id": "XBELBawoLlhL",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1714470318138,
     "user_tz": -180,
     "elapsed": 250,
     "user": {
      "displayName": "Никита Бобылев",
      "userId": "16665361595543873478"
     }
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Index Creation and Management\n",
    "Checks for the existence of a specific index and creates it if it doesn't exist. Uses the Pinecone client to manage indexes, setting up a new index with specified dimensions and metrics if necessary, and ensures the index is ready before proceeding. "
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "index_name = \"rag-example\"\n",
    "if index_name not in clientPine.list_indexes():\n",
    "\n",
    "    clientPine.create_index(\n",
    "        name=index_name,\n",
    "        dimension=len(embedded[0]),\n",
    "        metric='cosine',\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\",\n",
    "            region=\"us-east-1\"\n",
    "        ))\n",
    "\n",
    "    while not clientPine.describe_index(index_name).status['ready']:\n",
    "        time.sleep(1)"
   ],
   "metadata": {
    "id": "C2fwsbsB4PrY",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1714467579968,
     "user_tz": -180,
     "elapsed": 6174,
     "user": {
      "displayName": "Никита Бобылев",
      "userId": "16665361595543873478"
     }
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Data Upsert into Index\n",
    "Inserts or updates data in the vector index, associating PMIDs with their respective embedded vectors and metadata."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "index = clientPine.Index(index_name)\n",
    "upsert_data = [{\n",
    "    'id':item['pmid'],\n",
    "    \"metadata\":{\n",
    "        \"title\": item['title'],\n",
    "        \"abstract\": item['abstract']\n",
    "        },\n",
    "      \"values\": embedding,\n",
    "    } for item, embedding in zip(data, embedded)\n",
    "]\n",
    "print(\"Sample upsert data:\", next(iter(upsert_data)))\n",
    "index.upsert(vectors=upsert_data, namespace='example')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cPbaj6en_Qx2",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1714467586573,
     "user_tz": -180,
     "elapsed": 1619,
     "user": {
      "displayName": "Никита Бобылев",
      "userId": "16665361595543873478"
     }
    },
    "outputId": "de89bd08-268b-4609-c155-0e7200f673e2"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Initialize the ChatGPT chatbot\n",
    "Here, we set up the language model (ChatOpenAI) with predefined limits and stopping conditions, which are crucial for controlling the model's output during interactions."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "llm = ChatOpenAI(model_name='gpt-4', max_tokens=488,\n",
    "                 model_kwargs={\"stop\": [\"\\nQ:\", \"\\nA:\"]})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Setup directory loaders for handling CSV files"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "loaders = {\n",
    "    '.csv': DirectoryLoader(path=\"flattened_data\", glob=\"**/*_all.csv\")\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Langchain Pinecone vector storage instantiation \n",
    "This cell configures the methods for loading data and setting up a vector store with an embedding model. It's essential for enabling efficient data retrieval based on vector similarity."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "vectorstore = PineconeVectorStore(index_name='rag-example',\n",
    "                                  embedding=OpenAIEmbeddings(model='text-embedding-3-small'),\n",
    "                                  text_key='abstract')"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def get_prompt(instruction, examples, new_system_prompt):\n",
    "    SYSTEM_PROMPT = B_SYS + new_system_prompt + E_SYS\n",
    "    prompt_template =  SYSTEM_PROMPT + instruction  + \"\\n\" + examples\n",
    "    return prompt_template\n",
    "\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "sys_prompt = \"\"\"\\\n",
    "You are a helpful, respectful and honest assistant designed to assist with. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\"\n",
    "\n",
    "\n",
    "instruction = \"\"\"CONTEXT:/n/n {context}/n\n",
    "\"\"\"\n",
    "\n",
    "examples = \"\"\"\n",
    "Q: {question}\n",
    "A: \"\"\"\n",
    "template = get_prompt(instruction, examples, sys_prompt)\n",
    "print(template)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SFMqDoYOAvuZ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1714467598122,
     "user_tz": -180,
     "elapsed": 250,
     "user": {
      "displayName": "Никита Бобылев",
      "userId": "16665361595543873478"
     }
    },
    "outputId": "53eb1eb0-871f-4559-83b1-bc3607626b9f"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "QA_CHAIN_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=template,\n",
    ")"
   ],
   "metadata": {
    "id": "H70G40WKAs3S",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1714467630085,
     "user_tz": -180,
     "elapsed": 3,
     "user": {
      "displayName": "Никита Бобылев",
      "userId": "16665361595543873478"
     }
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "text_field = \"abstract\"\n",
    "vectorstore = PineconeVectorStore(index_name='rag-example',\n",
    "    embedding=OpenAIEmbeddings(model='text-embedding-3-small'),\n",
    "    text_key='abstract'\n",
    ")"
   ],
   "metadata": {
    "id": "PY-HWM8aCrnn",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1714470585175,
     "user_tz": -180,
     "elapsed": 2,
     "user": {
      "displayName": "Никита Бобылев",
      "userId": "16665361595543873478"
     }
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "query = \"Who can help me with AI questions? \"\n",
    "vectorstore.similarity_search(\n",
    "    query,\n",
    "    k=3\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-OSgGxBdCtPR",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1714470609744,
     "user_tz": -180,
     "elapsed": 617,
     "user": {
      "displayName": "Никита Бобылев",
      "userId": "16665361595543873478"
     }
    },
    "outputId": "265ba497-e95b-402e-a3f3-6aa85474a943"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "llm = ChatOpenAI(\n",
    "    openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "    model_name='gpt-3.5-turbo',\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever()\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x1GV48gHDzoM",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1714468885614,
     "user_tz": -180,
     "elapsed": 256,
     "user": {
      "displayName": "Никита Бобылев",
      "userId": "16665361595543873478"
     }
    },
    "outputId": "771815b1-9319-41a3-897e-5de63c2a2e65"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "llm = OpenAI(temperature=0)\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    ")"
   ],
   "metadata": {
    "id": "WDkheZ0M_VYb",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1714468902795,
     "user_tz": -180,
     "elapsed": 237,
     "user": {
      "displayName": "Никита Бобылев",
      "userId": "16665361595543873478"
     }
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "query = \"Who can help me with AI questions? \"\n",
    "qa_chain.run(query)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def chat_response(msg, history):\n",
    "    \"\"\"\n",
    "    Function to handle chat responses.\n",
    "    Args:\n",
    "        msg (str): The message to respond to.\n",
    "        history (str): The chat history.\n",
    "    Returns:\n",
    "        str: The chat response.\n",
    "    \"\"\"\n",
    "    return qa_chain({\"query\": msg})[\"result\"]\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Setup a Gradio interface for the application\n",
    "demo = gr.ChatInterface(chat_response)\n",
    "\n",
    "\n",
    "demo.launch()"
   ]
  }
 ]
}
